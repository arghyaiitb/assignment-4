# Assignment 4 - MNIST Digit Classification

## Model Overview

This project implements a Convolutional Neural Network (CNN) for MNIST digit classification using PyTorch.

### Model Architecture

The `Net` class defines the Convolutional Neural Network architecture:

```python
class Net(nn.Module):
    #This defines the structure of the NN.
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 8, kernel_size=3)
        self.conv2 = nn.Conv2d(8, 16, kernel_size=3)
        self.conv3 = nn.Conv2d(16, 8, kernel_size=3)
        self.fc1 = nn.Linear(5*5*8, 80)
        self.fc2 = nn.Linear(80, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))

        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2)

        x = F.relu(self.conv3(x))
        x = F.max_pool2d(x, 2)

        x = x.view(-1, 200)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)
```

-   **Input Layer**: Expects a single-channel image (e.g., 28x28x1 for MNIST).

-   **Convolutional Block 1**:
    -   `self.conv1 = nn.Conv2d(1, 8, kernel_size=3)`: A 2D convolutional layer taking 1 input channel and producing 8 output channels, using a 3x3 kernel.
    -   `F.relu(self.conv1(x))`: Applies the ReLU activation function to the output of `conv1`.

-   **Convolutional Block 2**:
    -   `self.conv2 = nn.Conv2d(8, 16, kernel_size=3)`: A 2D convolutional layer taking 8 input channels and producing 16 output channels, using a 3x3 kernel.
    -   `F.relu(self.conv2(x))`: Applies ReLU activation.
    -   `F.max_pool2d(x, 2)`: Applies a 2x2 max pooling operation, which reduces the spatial dimensions of the feature map by half.

-   **Convolutional Block 3**:
    -   `self.conv3 = nn.Conv2d(16, 8, kernel_size=3)`: A 2D convolutional layer taking 16 input channels and producing 8 output channels, using a 3x3 kernel.
    -   `F.relu(self.conv3(x))`: Applies ReLU activation.
    -   `F.max_pool2d(x, 2)`: Applies another 2x2 max pooling operation.

-   **Flattening Layer**:
    -   `x = x.view(-1, 5*5*8)`: Flattens the output of the convolutional layers into a 1D vector. The calculation `5*5*8` comes from the dimensions of the feature map after the two max-pooling operations (28 -> 26 -> 13 -> 11 -> 5, roughly after 3x3 convs and 2x2 pools) and the 8 output channels from `conv3`.

-   **Fully Connected Layers**:
    -   `self.fc1 = nn.Linear(5*5*8, 80)`: A fully connected layer mapping the 200 flattened features to 80 neurons.
    -   `F.relu(self.fc1(x))`: Applies ReLU activation.
    -   `self.fc2 = nn.Linear(80, 10)`: A final fully connected layer mapping the 80 neurons to 10 output neurons, corresponding to the 10 MNIST digit classes.

-   **Output Layer**:
    -   `return F.log_softmax(x, dim=1)`: Applies the log-softmax function to the output of `fc2`. This provides log-probabilities for each class, which is suitable for use with `nn.NLLLoss` (Negative Log Likelihood Loss) or `nn.CrossEntropyLoss` (which internally includes Log Softmax).

### Model Performance

-   **Total Parameters**: The model has 19,298 trainable parameters. This is calculated using the `count_parameters` function.
    ```python
    def count_parameters(model):
        return sum(p.numel() for p in model.parameters() if p.requires_grad)

    num_params = count_parameters(model)
    print(f"The model has {num_params} parameters.") # Output: The model has 19298 parameters.
    ```
-   **Test Accuracy**: Achieved **95.29%** on the training data (as per the note above regarding `test_loader` usage in the provided notebook). This indicates that the model is performing very well on the data it was trained on. To get an accurate measure of generalization, the model should be evaluated on the dedicated `test_data` using `test_transforms` for the `test_loader`.
-   **Dataset**: MNIST (28x28 grayscale images of handwritten digits 0-9).
-   **Training**: 1 epoch with SGD optimizer (lr=0.01, momentum=0.9).

### Data Augmentation

The model uses the following data augmentation techniques during training:
- Random center crop (22x22) with 10% probability
- Random rotation (Â±15 degrees)
- Normalization with MNIST statistics
